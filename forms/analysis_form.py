import streamlit as st
from typing import Dict, Any, List
from .base_form import BaseForm
from .analysis_components import (
    MetricsCalculator, 
    PerformanceAnalyzer, 
    EmergencyScenarioAnalyzer,
    AnalysisRenderer
)


class AnalysisForm(BaseForm):
    """
    Formulaire pour l'analyse des r√©sultats de simulation + r√©capitulatif.
    """
    
    def __init__(self):
        super().__init__("analysis")
        
        # Injection de d√©pendance - Composants sp√©cialis√©s
        self.calculator = MetricsCalculator()
        self.performance_analyzer = PerformanceAnalyzer()
        self.emergency_analyzer = EmergencyScenarioAnalyzer()
        self.renderer = AnalysisRenderer()
    
    def render(self, simulations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Rend le formulaire d'analyse avec les simulations.
        
        Args:
            simulations: Liste des simulations √† analyser
            
        Returns:
            Dict avec les donn√©es d'analyse compl√®tes
        """
        try:
            self.render_section_header(
                "üìà Statistiques et √©l√©ments pour l'analyse", 
                divider=True, 
                help_text="üí° Vous pouvez utiliser les donn√©es ci-dessous pour votre analyse"
            )
            
            # 1. CALCULS - D√©l√©gu√©s au MetricsCalculator
            metrics = self.calculator.calculate_basic_metrics(simulations)
            
            # 2. M√âTRIQUES PRINCIPALES - R√âDUCTIBLE üìä
            with st.expander("üìä M√©triques principales", expanded=True):
                self.renderer.render_metrics_display(metrics)
            
            # 3. ANALYSES AVANC√âES - R√âDUCTIBLE üî¨
            if simulations:
                with st.expander("üî¨ Analyses avanc√©es", expanded=False):
                    self._render_advanced_analysis_with_components(simulations, metrics)
            
            # 5. R√âCAPITULATIF - Toujours visible (le plus important)
            recapitulatif_data = self._render_summary_section()
            
            # 4. SAISIE UTILISATEUR - Toujours visible (le plus important)
            analysis_data = self._render_user_inputs(metrics)
            
            # 6. COMBINAISON DES DONN√âES
            complete_data = {
                **analysis_data,
                "recapitulatif_analyse": recapitulatif_data["recapitulatif_text"],
                "recapitulatif_metrics": recapitulatif_data["metrics"]
            }
            
            return complete_data
            
        except Exception as e:
            st.error(f"‚ùå Erreur dans le formulaire analyse : {str(e)}")
            return self._get_default_analysis()
    
    def _render_advanced_analysis_with_components(self, simulations: List[Dict[str, Any]], metrics: Dict[str, Any]) -> None:
        """
        Rend toutes les analyses avanc√©es en utilisant les composants sp√©cialis√©s.
        
        Args:
            simulations: Liste des simulations
            metrics: M√©triques calcul√©es
        """
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "üìä Performance", 
            "‚ùå Analyse des √©checs", 
            "üö® Sc√©narios d'urgence", 
            "üåä Conditions environnementales critiques", 
            "üö¢ R√©partition d√©taill√©e"
        ])
        
        with tab1:
            ship_analysis = self.performance_analyzer.analyze_ship_performance(simulations, metrics)
            maneuver_analysis = self.performance_analyzer.analyze_maneuver_performance(simulations, metrics)
            trends = self.performance_analyzer.identify_performance_trends(simulations, metrics)
            self.renderer.render_performance_analysis(ship_analysis, maneuver_analysis, trends)
        
        with tab2:
            echecs = [s for s in simulations if s.get("resultat") == "√âchec"]
            failure_analysis = self.performance_analyzer.analyze_failure_causes(echecs)
            self.renderer.render_failure_analysis(failure_analysis, echecs)
        
        with tab3:
            emergency_analysis = self.emergency_analyzer.analyze_emergency_scenarios(simulations)
            self.renderer.render_emergency_analysis(emergency_analysis)
        
        with tab4:
            conditions_analysis = self.calculator.analyze_environmental_conditions(simulations)
            self.renderer.render_environmental_conditions(conditions_analysis)
        
        with tab5:
            frequent_conditions = self.calculator.get_most_frequent_conditions(simulations, top_n=5) 
            self.renderer.render_detailed_distribution(simulations, frequent_conditions)
    
    def _render_user_inputs(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """
        Rend les champs de saisie sp√©cifiques au formulaire.
        
        Cette m√©thode reste dans le formulaire car elle g√®re la saisie utilisateur
        sp√©cifique √† ce contexte d'analyse.
        
        Args:
            metrics: M√©triques calcul√©es
            
        Returns:
            Dict avec les donn√©es saisies par l'utilisateur
        """
        objectifs_evaluations_data = self._render_objectives_evaluation()

        st.subheader("‚úèÔ∏è Saisie de donn√©es compl√©mentaires", divider=True)

        # R√©cup√©rer les valeurs par d√©faut
        defaults = self.get_defaults()
        
        col1, col2 = st.columns(2)
        with col1:
            # Conditions critiques identifi√©es
            conditions_critiques = self._render_conditions_critiques_input(defaults)
        with col2:
            # Distances et trajectoires
            distances_trajectoires = st.text_area(
                "üó∫Ô∏è Distances et trajectoires",
                value=defaults.get("distances_trajectoires", 
                    "Distance parcourue moyenne: 2.8 milles du pilotage √† l'accostage. "
                    "Temps moyen de man≈ìuvre: 45 minutes."
                ),
                help="Informations sur les distances parcourues et temps de man≈ìuvre",
                placeholder="Ex: Distance parcourue moyenne: 2.8 milles du pilotage √† l'accostage. "
                        "Temps moyen de man≈ìuvre: 45 minutes.",
                height=120
            )
                
        # Construire les donn√©es finales
        analysis_data = {
            "nombre_essais": metrics["nb_essais"],
            "taux_reussite": metrics["taux_reussite"],
            "conditions_critiques": conditions_critiques,
            "distances_trajectoires": distances_trajectoires,
            **objectifs_evaluations_data,
            # Ajouter les m√©triques d√©taill√©es pour compatibilit√©
            "metrics_detaillees": {
                "nb_reussis": metrics["nb_reussis"],
                "nb_echecs": metrics["nb_echecs"],
                "nb_urgences": metrics["nb_urgences"],
                "simulations_par_navire": metrics["simulations_par_navire"],
                "simulations_par_manoeuvre": metrics["simulations_par_manoeuvre"]
            }
        }
        
        return analysis_data
    
    def _render_summary_section(self) -> Dict[str, Any]:
        """
        Rend la section r√©capitulatif de l'analyse.
        
        Cette m√©thode reste dans le formulaire car elle g√®re l'√©tat de session
        et la saisie utilisateur sp√©cifique au r√©capitulatif.
        
        Returns:
            Dict avec le texte du r√©capitulatif et ses m√©triques
        """
        st.subheader("üìã Votre analyse", divider=True)
        
        # R√©cup√©rer la valeur existante depuis la session
        rapport_data = st.session_state.get("rapport_data", {})
        current_recapitulatif = (
            rapport_data.get("recapitulatif_analyse", "") or 
            rapport_data.get("conclusion", "")
        )
        
        # Zone de saisie du r√©capitulatif
        recapitulatif_text = st.text_area(
            "üìù R√©capitulatif g√©n√©ral de l'analyse",
            value=current_recapitulatif,
            height=120,
            placeholder=(
                "Ex: La pr√©sente √©tude de man≈ìuvrabilit√© d√©montre la faisabilit√© des op√©rations "
                "portuaires pour les porte-conteneurs ... Le taux de r√©ussite des simulations "
                "confirme que ..."
            ),
            help="Synth√®se g√©n√©rale des r√©sultats de votre analyse de man≈ìuvrabilit√©"
        )
        
        recap_metrics = self._calculate_text_metrics(recapitulatif_text)        
        if recapitulatif_text:
            self._display_text_metrics(recap_metrics)
            
            # Analyse du contenu avec le renderer
            with st.expander("üîç Analyse du contenu", expanded=False):
                self.renderer.render_content_analysis(recapitulatif_text)
        
        return {
            "recapitulatif_text": recapitulatif_text,
            "metrics": recap_metrics
        }
    
    def _render_conditions_critiques_input(self, defaults: Dict[str, Any]) -> List[str]:
        """
        Rend le champ de saisie des conditions critiques.
        
        Args:
            defaults: Valeurs par d√©faut
            
        Returns:
            Liste des conditions critiques
        """
        # R√©cup√©rer les conditions existantes
        rapport_data = st.session_state.get("rapport_data", {})
        existing_conditions = (
            rapport_data.get("analyse_synthese", {}).get("conditions_critiques", [])
        )
        conditions_text = "\n".join(existing_conditions) if existing_conditions else ""
        
        conditions_input = st.text_area(
            "‚ö†Ô∏è Conditions critiques identifi√©es",
            value=conditions_text,
            help="Seuils et conditions qui ont rendu les man≈ìuvres difficiles ou impossibles (une condition par ligne)",
            placeholder="Ex:\nVent > 25 n≈ìuds\nMar√©e basse < 2m\nHoule > 1.5m",
            height=120
        )
        
        # Convertir en liste
        conditions_list = [c.strip() for c in conditions_input.split("\n") if c.strip()]
        
        # Afficher le d√©compte
        if conditions_list:
            st.caption(f"üìã {len(conditions_list)} condition(s) critique(s) identifi√©e(s)")
        
        return conditions_list
    
    def _calculate_text_metrics(self, text: str) -> Dict[str, Any]:
        """
        Calcule les m√©triques d'un texte.
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Dict avec les m√©triques du texte
        """
        if not text:
            return {"caracteres": 0, "mots": 0, "temps_lecture": 0}
        
        nb_caracteres = len(text)
        nb_mots = len(text.split())
        temps_lecture = max(1, nb_mots // 200)  # ~200 mots/min
        
        return {
            "caracteres": nb_caracteres,
            "mots": nb_mots,
            "temps_lecture": temps_lecture
        }
    
    def _display_text_metrics(self, metrics: Dict[str, Any]) -> None:
        """
        Affiche les m√©triques d'un texte.
        
        Args:
            metrics: M√©triques calcul√©es
        """
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.caption(f"‚úÖ {metrics['caracteres']} caract√®res")
        with col2:
            st.caption(f"üìÑ {metrics['mots']} mots")
        with col3:
            st.caption(f"‚è±Ô∏è ~{metrics['temps_lecture']} min de lecture")
    
    def _render_objectives_evaluation(self) -> Dict[str, Any]:
        """
        Rend la section d'√©valuation des objectifs d√©finis dans l'introduction.
        
        Cette m√©thode r√©cup√®re les objectifs depuis l'introduction et cr√©e
        des pav√©s de commentaire pour √©valuer le degr√© de r√©alisation de chacun.
        
        Returns:
            Dict avec les √©valuations des objectifs
        """
        st.subheader("üéØ √âvaluation des objectifs de l'√©tude", divider=True)
        
        # R√©cup√©rer les objectifs depuis l'introduction
        rapport_data = st.session_state.get("rapport_data", {})
        introduction_data = rapport_data.get("introduction", {})
        objectifs_text = introduction_data.get("objectifs", "")
        
        if not objectifs_text.strip():
            st.info("üí° Aucun objectif d√©fini dans l'introduction. Retournez √† l'onglet Introduction pour en ajouter.")
            return {"objectifs_evaluations": {}}
        
        # Parser les objectifs (un par ligne)
        objectifs_list = [obj.strip() for obj in objectifs_text.split("\n") if obj.strip()]
        
        if not objectifs_list:
            st.info("üí° Objectifs non format√©s correctement. S√©parez chaque objectif par une ligne dans l'introduction.")
            return {"objectifs_evaluations": {}}
        
        st.info(f"üìã √âvaluez la r√©alisation des {len(objectifs_list)} objectifs d√©finis dans l'introduction")
        
        # R√©cup√©rer les √©valuations existantes
        existing_evaluations = rapport_data.get("analyse_synthese", {}).get("objectifs_evaluations", {})
        
        evaluations = {}
        
        # Cr√©er les pav√©s d'√©valuation pour chaque objectif
        for i, objectif in enumerate(objectifs_list, 1):
            # Tronquer l'objectif pour l'affichage dans l'en-t√™te
            objectif_court = objectif[:80] + "..." if len(objectif) > 80 else objectif
            
            with st.expander(f"üìå Objectif {i}: {objectif_court}", expanded=True):
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    # Afficher l'objectif complet
                    st.write("**Objectif :**")
                    st.write(f"*{objectif}*")
                
                with col2:
                    # Degr√© de r√©alisation (s√©lecteur rapide)
                    degre_options = [
                        "Non √©valu√©",
                        "‚úÖ Objectif atteint",
                        "üü° Partiellement atteint", 
                        "üî¥ Non atteint",
                        "‚ûï D√©pass√©"
                    ]
                    
                    # R√©cup√©rer la valeur existante
                    existing_degre = existing_evaluations.get(f"objectif_{i}", {}).get("degre_realisation", "Non √©valu√©")
                    default_index = degre_options.index(existing_degre) if existing_degre in degre_options else 0
                    
                    degre_realisation = st.selectbox(
                        "Degr√© de r√©alisation",
                        degre_options,
                        index=default_index,
                        key=f"degre_obj_{i}"
                    )
                
                # Zone de commentaire d√©taill√©
                existing_comment = existing_evaluations.get(f"objectif_{i}", {}).get("commentaire", "")
                
                commentaire = st.text_area(
                    "üí¨ Commentaire d√©taill√© sur la r√©alisation",
                    value=existing_comment,
                    height=120,
                    placeholder=self._get_placeholder_for_degree(degre_realisation),
                    key=f"comment_obj_{i}",
                    help="Analysez comment cet objectif a √©t√© atteint gr√¢ce aux simulations et analyses"
                )
                
                # M√©triques li√©es (optionnel)
                metrics_complement = st.text_input(
                    "üìä M√©triques de support (optionnel)",
                    value=existing_evaluations.get(f"objectif_{i}", {}).get("metriques", ""),
                    placeholder="Ex: Taux de r√©ussite 85%, 23 simulations valid√©es...",
                    key=f"metrics_obj_{i}",
                    help="Donn√©es chiffr√©es qui appuient votre √©valuation"
                )
                
                # Stocker l'√©valuation
                evaluations[f"objectif_{i}"] = {
                    "objectif_texte": objectif,
                    "degre_realisation": degre_realisation,
                    "commentaire": commentaire,
                    "metriques": metrics_complement,
                    "ordre": i
                }
                
                # Affichage du r√©sum√© si rempli
                if commentaire.strip() or degre_realisation != "Non √©valu√©":
                    with st.container():
                        if commentaire.strip():
                            char_count = len(commentaire.strip())
                            st.caption(f"**Commentaire:** {char_count} caract√®res")
        
        # R√©sum√© global des √©valuations
        self._render_objectives_summary(evaluations)
        
        return {"objectifs_evaluations": evaluations}

    def _get_placeholder_for_degree(self, degre: str) -> str:
        """
        G√©n√®re un placeholder contextuel selon le degr√© de r√©alisation.
        
        Args:
            degre: Degr√© de r√©alisation s√©lectionn√©
            
        Returns:
            Texte de placeholder adapt√©
        """
        placeholders = {
            "‚úÖ Objectif atteint": "Ex: Les simulations d√©montrent que... Les r√©sultats confirment...",
            "üü° Partiellement atteint": "Ex: Objectif atteint dans 70% des cas, mais limit√© par... Des am√©liorations sont n√©cessaires pour...",
            "üî¥ Non atteint": "Ex: Les simulations r√©v√®lent que... Les contraintes identifi√©es sont... Il faudra modifier...",
            "‚ûï D√©pass√©": "Ex: Non seulement l'objectif est atteint, mais en plus... Les r√©sultats d√©passent les attentes car...",
            "Non √©valu√©": "Ex: D√©crivez dans quelle mesure cet objectif a √©t√© atteint gr√¢ce √† votre √©tude..."
        }
        
        return placeholders.get(degre, "Commentez la r√©alisation de cet objectif...")

    def _render_objectives_summary(self, evaluations: Dict[str, Any]) -> None:
        """
        Rend un r√©sum√© visuel des √©valuations d'objectifs.
        
        Args:
            evaluations: Dict des √©valuations d'objectifs
        """
        if not evaluations:
            return
        
        st.subheader("üìä R√©sum√© des √©valuations", divider=True)
        
        # Compter les degr√©s de r√©alisation
        degres_count = {}
        total_objectifs = len(evaluations)
        
        for eval_data in evaluations.values():
            degre = eval_data.get("degre_realisation", "Non √©valu√©")
            degres_count[degre] = degres_count.get(degre, 0) + 1
        
        # Affichage en colonnes
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            atteints = degres_count.get("‚úÖ Objectif atteint", 0)
            st.metric("Objectifs atteints", atteints, f"{atteints/total_objectifs*100:.0f}%" if total_objectifs > 0 else "0%")
        
        with col2:
            partiels = degres_count.get("üü° Partiellement atteint", 0)
            st.metric("Partiellement", partiels, f"{partiels/total_objectifs*100:.0f}%" if total_objectifs > 0 else "0%")
        
        with col3:
            non_atteints = degres_count.get("üî¥ Non atteint", 0)
            st.metric("Non atteints", non_atteints, f"{non_atteints/total_objectifs*100:.0f}%" if total_objectifs > 0 else "0%")
        
        with col4:
            depasses = degres_count.get("‚ûï D√©pass√©", 0)
            st.metric("D√©pass√©s", depasses, f"{depasses/total_objectifs*100:.0f}%" if total_objectifs > 0 else "0%")
        
        # Barre de progression globale
        objectifs_evalues = sum(1 for eval_data in evaluations.values() 
                            if eval_data.get("degre_realisation", "Non √©valu√©") != "Non √©valu√©")
        
        progress = objectifs_evalues / total_objectifs if total_objectifs > 0 else 0
        st.progress(progress, text=f"Objectifs √©valu√©s: {objectifs_evalues}/{total_objectifs}")
        
        # Recommandations selon les r√©sultats
        if progress < 0.5:
            st.warning("‚ö†Ô∏è Peu d'objectifs √©valu√©s. Compl√©tez les √©valuations pour une analyse exhaustive.")
        elif non_atteints > total_objectifs * 0.3:
            st.error("üî¥ Plus de 30% des objectifs ne sont pas atteints. R√©vision des hypoth√®ses recommand√©e.")
        elif atteints + depasses >= total_objectifs * 0.8:
            st.success("üéâ Plus de 80% des objectifs sont atteints ou d√©pass√©s. Excellent r√©sultat !")
        else:
            st.info("üìà R√©sultats mitig√©s. Analysez les objectifs partiellement atteints pour des am√©liorations.")

    
    def _get_default_analysis(self) -> Dict[str, Any]:
        """
        Retourne une analyse par d√©faut en cas d'erreur.
        
        Returns:
            Dict avec des valeurs par d√©faut
        """
        return {
            "nombre_essais": 0,
            "taux_reussite": 0.0,
            "conditions_critiques": [],
            "distances_trajectoires": "",
            "commentaire": "",
            "recapitulatif_analyse": "",
            "recapitulatif_metrics": {}
        }
    
    def get_analysis_summary(self, simulations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        G√©n√®re un r√©sum√© complet de l'analyse pour l'export.
        
        Cette m√©thode publique permet d'obtenir toutes les analyses
        sans rendu Streamlit, utile pour l'export ou les tests.
        
        Args:
            simulations: Liste des simulations √† analyser
            
        Returns:
            Dict avec le r√©sum√© complet de l'analyse
        """
        try:
            # Calculs de base
            metrics = self.calculator.calculate_basic_metrics(simulations)
            
            # Analyses sp√©cialis√©es
            ship_analysis = self.performance_analyzer.analyze_ship_performance(simulations, metrics)
            maneuver_analysis = self.performance_analyzer.analyze_maneuver_performance(simulations, metrics)
            trends = self.performance_analyzer.identify_performance_trends(simulations, metrics)
            
            echecs = [s for s in simulations if s.get("resultat") == "√âchec"]
            failure_analysis = self.performance_analyzer.analyze_failure_causes(echecs)
            
            emergency_analysis = self.emergency_analyzer.analyze_emergency_scenarios(simulations)
            emergency_stats = self.emergency_analyzer.get_emergency_statistics(simulations)
            
            conditions_analysis = self.calculator.analyze_environmental_conditions(simulations)
            
            # R√©sum√© structur√©
            summary = {
                "basic_metrics": metrics,
                "performance_analysis": {
                    "ships": ship_analysis,
                    "maneuvers": maneuver_analysis,
                    "trends": trends
                },
                "failure_analysis": failure_analysis,
                "emergency_analysis": emergency_analysis,
                "emergency_statistics": emergency_stats,
                "environmental_conditions": conditions_analysis,
                "overall_assessment": self._generate_overall_assessment(metrics, emergency_stats),
                "export_data": self.renderer.render_export_summary(metrics, {}),
                "generated_at": self._get_current_timestamp()
            }
            
            return summary
            
        except Exception as e:
            # Retour d√©grad√© en cas d'erreur
            return {
                "error": str(e),
                "basic_metrics": self._get_default_analysis(),
                "generated_at": self._get_current_timestamp()
            }
    
    def _generate_overall_assessment(self, metrics: Dict[str, Any], emergency_stats: Dict[str, Any]) -> Dict[str, str]:
        """
        G√©n√®re une √©valuation globale de l'analyse.
        
        Args:
            metrics: M√©triques de base
            emergency_stats: Statistiques d'urgence
            
        Returns:
            Dict avec l'√©valuation globale
        """
        success_rate = metrics["taux_reussite"]
        emergency_rate = emergency_stats.get("emergency_rate", 0)
        
        # √âvaluation de la performance
        if success_rate >= 0.9:
            performance_level = "excellente"
            performance_color = "üü¢"
        elif success_rate >= 0.8:
            performance_level = "tr√®s bonne"
            performance_color = "üü¢"
        elif success_rate >= 0.7:
            performance_level = "bonne"
            performance_color = "üü°"
        elif success_rate >= 0.6:
            performance_level = "satisfaisante"
            performance_color = "üü°"
        else:
            performance_level = "pr√©occupante"
            performance_color = "üî¥"
        
        # √âvaluation du risque
        if emergency_rate > 0.3 or success_rate < 0.5:
            risk_level = "√©lev√©"
            risk_color = "üî¥"
        elif emergency_rate > 0.2 or success_rate < 0.7:
            risk_level = "mod√©r√©"
            risk_color = "üü°"
        else:
            risk_level = "faible"
            risk_color = "üü¢"
        
        # Recommandation g√©n√©rale
        if success_rate >= 0.8 and emergency_rate < 0.1:
            recommendation = "Les op√©rations peuvent √™tre men√©es en toute s√©curit√©"
        elif success_rate >= 0.6:
            recommendation = "Des am√©liorations sont recommand√©es avant d√©ploiement"
        else:
            recommendation = "R√©vision majeure des proc√©dures n√©cessaire"
        
        return {
            "performance_level": performance_level,
            "performance_color": performance_color,
            "risk_level": risk_level,
            "risk_color": risk_color,
            "overall_recommendation": recommendation,
            "confidence": "√©lev√©e" if metrics["nb_essais"] >= 20 else "mod√©r√©e"
        }
    
    def _get_current_timestamp(self) -> str:
        """Retourne le timestamp actuel."""
        from datetime import datetime
        return datetime.now().isoformat()
