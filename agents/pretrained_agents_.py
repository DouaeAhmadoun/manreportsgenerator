import json
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from jinja2 import Template

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# RÃ¨gles de formatage communes injectÃ©es dans les prompts
COMMON_FORMAT_RULES = (
    "- Pas de titres ni de HMTL ni de Markdown\n"
    "- Pas de mise en forme (gras, listes longues)\n"
    "- Style professionnel, phrases courtes\n"
    "- N'invente pas de donnÃ©es absentes du contexte\n"
    "- Ã‰cris en franÃ§ais professionnel, phrases fluides\n"
    "- Commence directement par le contenu (pas de titre)\n"
    "- Utilise un vocabulaire technique maritime prÃ©cis\n"
)


class PretrainedManager:
    """Gestionnaire d'agents prÃ©-entraÃ®nÃ©s granulaires"""
    
    def __init__(self, cache_directory: str = "agents/training_cache", verbose: bool = True):
        self.cache_directory = Path(cache_directory)
        self.verbose = verbose
        self.debug_run_dir: Optional[Path] = None
        
        # Fichiers de cache
        self.training_cache = self.cache_directory / "training_data_granular.json"
        self.metadata_cache = self.cache_directory / "training_metadata_granular.json"
        
        # DonnÃ©es chargÃ©es
        self.training_data = None
        self.metadata = None
        
        # Configuration
        self.default_model = "openai/gpt-oss-20b:free"
        self.timeout = 30
        
        # Mapping des sections vers les gÃ©nÃ©rateurs
        self.section_generators = {
            "navires": self._generate_navires,
            "introduction": self._generate_introduction,
            "analyse": self._generate_analyse,
            "analyse_stats": self._generate_analyse_stats,
            "analyse_recommandations": self._generate_analyse_recommandations,
            "donnees_entree_intro": self._generate_donnees_entree_intro,
            "donnees_entree_plan_masse": self._generate_donnees_entree_plan_masse,
            "donnees_entree_bathymetrie": self._generate_donnees_entree_bathymetrie,
            "donnees_entree_balisage": self._generate_donnees_entree_balisage,
            "donnees_entree_conditions_intro": self._generate_donnees_entree_conditions_intro,
            "donnees_entree_houle": self._generate_donnees_entree_houle,
            "donnees_entree_vent": self._generate_donnees_entree_vent,
            "donnees_entree_courant": self._generate_donnees_entree_courant,
            "donnees_entree_maree": self._generate_donnees_entree_maree,
            "donnees_entree_agitation": self._generate_donnees_entree_agitation,
            "donnees_entree_synthese": self._generate_donnees_entree_synthese,
            #"navires": self._generate_navires,
            "remorqueurs": self._generate_remorqueurs,
            "simulations": self._generate_simulations,
            "scenarios_urgence": self._generate_scenarios_urgence,
        }
        
        self._log(f"ğŸ¤– PretrainedManager initialisÃ© ({len(self.section_generators)} gÃ©nÃ©rateurs)")
    
    def load_training_data(self) -> bool:
        """Charge les donnÃ©es d'entraÃ®nement"""
        try:
            if self.training_cache.exists():
                with open(self.training_cache, 'r', encoding='utf-8') as f:
                    self.training_data = json.load(f)
            
            if self.metadata_cache.exists():
                with open(self.metadata_cache, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
            
            # VÃ©rifier le type
            if self.metadata and self.metadata.get("training_type") != "granular_v2":
                self._log("âš ï¸ DonnÃ©es non granulaires - RÃ©-entraÃ®nement recommandÃ©")
                return False
            
            if not self.training_data:
                self._log("âŒ Aucune donnÃ©e d'entraÃ®nement")
                return False
            
            self._log(f"âœ… DonnÃ©es chargÃ©es: {len(self.training_data)} sections")
            return True
            
        except Exception as e:
            self._log(f"âŒ Erreur chargement: {e}")
            return False
    
    def generate_granular_sections_for_report(self, 
                                              rapport_data: dict, 
                                              sections_requested: List[str] = None,
                                              progress_mgr=None,
                                              progress_range=None) -> dict:
        """
        GÃ©nÃ¨re les sections avec les agents prÃ©-entraÃ®nÃ©s
        
        Args:
            rapport_data: DonnÃ©es du rapport
            sections_requested: Sections spÃ©cifiques (None = toutes)
            
        Returns:
            Dict {section_name: generated_content}
        """
        if not self.load_training_data():
            self._log("âš ï¸ Mode fallback activÃ©")
            return self._generate_fallback_sections(rapport_data)
        
        # PrÃ©parer un dossier de debug par gÃ©nÃ©ration
        run_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.debug_run_dir = Path("debug/prompt_renders") / run_ts
        try:
            self.debug_run_dir.mkdir(parents=True, exist_ok=True)
            self._log(f"[debug] prompts â†’ {self.debug_run_dir}")
        except Exception as e:
            self._log(f"[debug] impossible de crÃ©er le dossier de dump: {e}")
            self.debug_run_dir = None
        
        # Sections Ã  gÃ©nÃ©rer
        if sections_requested is None:
            sections = list(self.section_generators.keys())
        else:
            sections = [s for s in sections_requested if s in self.section_generators]
        
        self._log(f"ğŸš€ GÃ©nÃ©ration de {len(sections)} sections...")
        if progress_mgr:
            progress_mgr.log_detail(f"GÃ©nÃ©ration IA: {len(sections)} sections Ã  traiter")
        start_p, end_p = progress_range if progress_range else (20, 65)
        steps_count = max(1, len(sections))
        step_increment = max(1, int((end_p - start_p) / steps_count))
        current_step = start_p
        
        generated = {}
        stats = {"success": 0, "fallback": 0, "failed": 0}
        
        for section_name in sections:
            try:
                self._log(f"  ğŸ”„ {section_name}...")
                if progress_mgr:
                    progress_mgr.log_detail(f"GÃ©nÃ©ration de la section {section_name} en cours...")
                    progress_mgr.update(current_step, "GÃ©nÃ©ration IA", f"âœï¸ {section_name} en cours...")
                
                generator = self.section_generators[section_name]
                content = generator(rapport_data)
                
                if content and len(content.strip()) > 20:
                    generated[section_name] = content
                    stats["success"] += 1
                    self._log(f"  âœ… {section_name}: {len(content)} chars")
                    if progress_mgr:
                        progress_mgr.log_detail(f"Section {section_name} gÃ©nÃ©rÃ©e ({len(content)} caractÃ¨res)")
                    # Dump du texte gÃ©nÃ©rÃ©
                    try:
                        if self.debug_run_dir:
                            gen_path = self.debug_run_dir / f"{section_name}_generated.txt"
                            gen_path.write_text(content, encoding="utf-8")
                    except Exception:
                        pass
                else:
                    # Fallback
                    fallback = self._get_fallback_for_section(section_name, rapport_data)
                    generated[section_name] = fallback
                    stats["fallback"] += 1
                    self._log(f"  âš ï¸ {section_name}: fallback")
                    if progress_mgr:
                        progress_mgr.log_detail(f"Section {section_name} en fallback ({len(fallback)} caractÃ¨res)")
                    try:
                        if self.debug_run_dir:
                            gen_path = self.debug_run_dir / f"{section_name}_generated.txt"
                            gen_path.write_text(fallback, encoding="utf-8")
                    except Exception:
                        pass
                    
            except Exception as e:
                self._log(f"  âŒ {section_name}: {e}")
                stats["failed"] += 1
                
                # Essayer fallback
                try:
                    generated[section_name] = self._get_fallback_for_section(section_name, rapport_data)
                    stats["fallback"] += 1
                    if progress_mgr:
                        progress_mgr.log_detail(f"Section {section_name} fallback suite erreur ({e})")
                except:
                    pass
            finally:
                if progress_mgr:
                    current_step = min(end_p, current_step + step_increment)
        
        if progress_mgr:
            progress_mgr.update(end_p, "GÃ©nÃ©ration IA", "âœ… Sections IA traitÃ©es")
        
        self._print_stats(stats, len(sections))
        self.debug_run_dir = None
        return generated
    
    # =========================================================================
    # APPEL API
    # =========================================================================
    
    def _generate_with_ai(self, section_name: str, prompt: str) -> Optional[str]:
        """Appelle l'API OpenRouter"""
        if not REQUESTS_AVAILABLE:
            return None
        
        try:
            response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {self._get_api_key()}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": self.default_model,
                    "messages": [
                        {
                            "role": "system", 
                            "content": "Tu es un expert en ingÃ©nierie maritime et rÃ©daction de rapports techniques. RÃ©ponds uniquement avec le contenu demandÃ©, sans prÃ©ambule."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": 1500,
                    "temperature": 0.3
                },
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                if content and len(content.strip()) > 20:
                    return content.strip()
            elif response.status_code == 429:
                print(f"    âš ï¸ Rate limit pour {section_name}")
                time.sleep(2)
            else:
                print(f"    âš ï¸ API Error {response.status_code}")
                
        except Exception as e:
            print(f"    âš ï¸ Erreur API: {e}")
        
        return None
    
    def _get_api_key(self) -> str:
        """RÃ©cupÃ¨re la clÃ© API"""
        key = os.getenv("OPENROUTER_API_KEY")
        if key:
            return key
        
        try:
            from config import get_openrouter_key
            return get_openrouter_key()
        except ImportError:
            return ""
    
    # =========================================================================
    # CONSTRUCTION DES PROMPTS
    # =========================================================================
    
    def _build_prompt(self, section_name: str, rapport_data: dict) -> str:
        """Construit le prompt pour une section"""
        # Essayer de charger depuis fichier
        prompt_file = Path("prompts") / f"{section_name}.txt"

        # PrÃ©parer un contexte enrichi avec les alias attendus par certains prompts
        prompt_context = dict(rapport_data)
        prompt_context["analyse_synthese"] = rapport_data.get("analyse_synthese", {})
        donnees_navires = rapport_data.get("donnees_navires", {})
        prompt_context["navires_liste"] = donnees_navires.get("navires", {}).get("navires", [])
        prompt_context["remorqueurs_liste"] = donnees_navires.get("remorqueurs", {}).get("remorqueurs", [])
        
        if prompt_file.exists():
            try:
                content = prompt_file.read_text(encoding='utf-8')
                template = Template(content)
                rendered = template.render(**prompt_context, regles_formatage=COMMON_FORMAT_RULES)
                # Dump du prompt rendu pour inspection
                try:
                    out_dir = self.debug_run_dir if self.debug_run_dir else Path("debug/prompt_renders")
                    out_dir.mkdir(parents=True, exist_ok=True)
                    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                    out_path = out_dir / f"{section_name}_{ts}.txt"
                    out_path.write_text(rendered, encoding="utf-8")
                except Exception as dump_e:
                    pass
                return rendered
            except Exception as e:
                print(f"    âš ï¸ Erreur prompt {section_name}: {e}")
        else:
            print(f"    âš ï¸ Prompt {section_name} doesn't exist")
        
        return self._get_fallback_prompt(section_name, rapport_data)

    def _get_fallback_prompt(self, section_name: str, rapport_data: dict) -> str:
        """Prompt de fallback simple basÃ© sur le titre du rapport"""
        meta = rapport_data.get("metadonnees", {})
        titre = meta.get("titre", "Ã‰tude de manÅ“uvrabilitÃ©")
        
        prompts = {
            "introduction": f"RÃ©dige l'introduction pour un rapport de manÅ“uvrabilitÃ© intitulÃ© \"{titre}\". Inclus: contexte, objectifs, mÃ©thodologie.",
            "donnees_entree_bathymetrie": f"PrÃ©sente les donnÃ©es bathymÃ©triques pour l'Ã©tude \"{titre}\". Inclus: source, profondeurs, caractÃ©ristiques.",
            "donnees_entree_conditions_intro": f"RÃ©dige l'introduction des conditions environnementales pour l'Ã©tude \"{titre}\". PrÃ©sente l'influence de la houle, du vent, des courants et des marÃ©es sur la manÅ“uvrabilitÃ©.",
            "donnees_entree_houle": f"DÃ©cris les conditions de houle pour l'Ã©tude \"{titre}\". Inclus: hauteurs, pÃ©riodes, directions.",
            "navires": f"PrÃ©sente les navires sÃ©lectionnÃ©s pour l'Ã©tude \"{titre}\". Inclus: types, caractÃ©ristiques, justification.",
            "simulations": f"DÃ©cris la mÃ©thodologie des simulations pour l'Ã©tude \"{titre}\". Inclus: approche, paramÃ¨tres, conditions.",
            "analyse": f"Analyse les rÃ©sultats de l'Ã©tude \"{titre}\". Inclus: synthÃ¨se, conditions critiques, recommandations.",
        }
        
        return prompts.get(section_name, f"RÃ©dige la section {section_name} pour l'Ã©tude {titre}.")

    # =========================================================================
    # GÃ‰NÃ‰RATEURS PAR SECTION
    # =========================================================================
    
    def _generate_introduction(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("introduction", rapport_data)
        return self._generate_with_ai("introduction", prompt) or self._fallback_introduction(rapport_data)
    
    def _generate_analyse(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("analyse", rapport_data)
        return self._generate_with_ai("analyse", prompt) or self._fallback_analyse(rapport_data)

    def _generate_analyse_stats(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("analyse_stats", rapport_data)
        return self._generate_with_ai("analyse_stats", prompt) or \
            "Statistiques clÃ©s des simulations : taux de rÃ©ussite global, rÃ©partition par manÅ“uvre et par navire, identification des conditions critiques."

    def _generate_analyse_recommandations(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("analyse_recommandations", rapport_data)
        return self._generate_with_ai("analyse_recommandations", prompt) or \
            "Recommandations principales : adapter les seuils mÃ©tÃ©o (vent, houle, courant), configurer lâ€™assistance remorqueurs selon le navire, renforcer la vigilance sur les scÃ©narios critiques."
    
    def _generate_donnees_entree_intro(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("donnees_entree_intro", rapport_data)
        return self._generate_with_ai("donnees_entree_intro", prompt) or \
            "Cette section prÃ©sente l'ensemble des donnÃ©es d'entrÃ©e utilisÃ©es pour l'Ã©tude de manÅ“uvrabilitÃ©."
    
    def _generate_donnees_entree_plan_masse(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("donnees_entree_plan_masse", rapport_data)
        return self._generate_with_ai("donnees_entree_plan_masse", prompt) or \
            "Le plan de masse prÃ©sente l'amÃ©nagement portuaire Ã©tudiÃ© avec les diffÃ©rentes infrastructures."
    
    def _generate_donnees_entree_bathymetrie(self, rapport_data: dict) -> str:
        donnees = rapport_data.get("donnees_entree", {}).get("bathymetrie", {})
        prompt = self._build_prompt("donnees_entree_bathymetrie", rapport_data)
        result = self._generate_with_ai("donnees_entree_bathymetrie", prompt)
        return result or f"La bathymÃ©trie du site a Ã©tÃ© Ã©tablie Ã  partir de {donnees.get('source', 'relevÃ©s rÃ©cents')}."
    
    def _generate_donnees_entree_balisage(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("donnees_entree_balisage", rapport_data)
        return self._generate_with_ai("donnees_entree_balisage", prompt) or \
            "Le plan de balisage dÃ©finit la signalisation maritime et les aides Ã  la navigation."
    
    def _generate_donnees_entree_conditions_intro(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("donnees_entree_conditions_intro", rapport_data)
        return self._generate_with_ai("donnees_entree_conditions_intro", prompt) or \
            "Les conditions environnementales influencent directement la manÅ“uvrabilitÃ© des navires."
    
    def _generate_donnees_entree_houle(self, rapport_data: dict) -> str:
        conditions = rapport_data.get("donnees_entree", {}).get("conditions_environnementales", {})
        houle = conditions.get("houle", {})
        prompt = self._build_prompt("donnees_entree_houle", rapport_data)
        result = self._generate_with_ai("donnees_entree_houle", prompt)
        return result or f"Les conditions de houle retenues sont: {houle.get('valeurs_retenues', 'Hs = 2.5m')}."
    
    def _generate_donnees_entree_vent(self, rapport_data: dict) -> str:
        conditions = rapport_data.get("donnees_entree", {}).get("conditions_environnementales", {})
        vent = conditions.get("vent", {})
        prompt = self._build_prompt("donnees_entree_vent", rapport_data)
        result = self._generate_with_ai("donnees_entree_vent", prompt)
        return result or f"Les conditions de vent considÃ©rÃ©es sont: {vent.get('valeurs_retenues', '30 kts')}."
    
    def _generate_donnees_entree_courant(self, rapport_data: dict) -> str:
        conditions = rapport_data.get("donnees_entree", {}).get("conditions_environnementales", {})
        courant = conditions.get("courant", {})
        prompt = self._build_prompt("donnees_entree_courant", rapport_data)
        result = self._generate_with_ai("donnees_entree_courant", prompt)
        return result or f"Les conditions de courant retenues sont: {courant.get('valeurs_retenues', '1.5 kts')}."
    
    def _generate_donnees_entree_maree(self, rapport_data: dict) -> str:
        conditions = rapport_data.get("donnees_entree", {}).get("conditions_environnementales", {})
        maree = conditions.get("maree", {})
        prompt = self._build_prompt("donnees_entree_maree", rapport_data)
        result = self._generate_with_ai("donnees_entree_maree", prompt)
        return result or f"Les conditions de marÃ©e considÃ©rÃ©es sont: {maree.get('valeurs_retenues', 'Marnage 4.2m')}."
    
    def _generate_donnees_entree_agitation(self, rapport_data: dict) -> str:
        conditions = rapport_data.get("donnees_entree", {}).get("conditions_environnementales", {})
        agitation = conditions.get("agitation", {})
        prompt = self._build_prompt("donnees_entree_agitation", rapport_data)
        result = self._generate_with_ai("donnees_entree_agitation", prompt)
        return result or f"L'agitation rÃ©siduelle est estimÃ©e Ã : {agitation.get('valeurs_retenues', '0.5m')}."
    
    def _generate_donnees_entree_synthese(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("donnees_entree_synthese", rapport_data)
        return self._generate_with_ai("donnees_entree_synthese", prompt) or \
            "La synthÃ¨se des donnÃ©es environnementales permet de dÃ©finir les conditions reprÃ©sentatives."
    
    def _generate_navires(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("navires", rapport_data)
        # Tentative IA
        ai_result = self._generate_with_ai("navires", prompt)
        if ai_result:
            # Si l'IA dit qu'elle n'a pas de donnÃ©es, basculer sur le fallback enrichi
            ai_lower = ai_result.lower()
            blockers = [
                "aucune donnÃ©e",
                "aucune information",
                "je ne dispose pas",
                "je ne dispose dâ€™aucune",
                "je ne peux pas",
                "iâ€™m sorry"
            ]
            if not any(b in ai_lower for b in blockers):
                return ai_result
    
        # Fallback enrichi Ã  partir des donnÃ©es disponibles
        navires = rapport_data.get("donnees_navires", {}).get("navires", {}).get("navires", [])
        if navires:
            lignes = []
            for nav in navires[:5]:
                nom = nav.get("nom", "Navire")
                type_nav = nav.get("type", "Type")
                parts = [f"{nom} ({type_nav})"]
                if nav.get("longueur"):
                    parts.append(f"{nav['longueur']} m")
                if nav.get("tirant_eau_av"):
                    parts.append(f"TE {nav['tirant_eau_av']} m")
                lignes.append(" - " + ", ".join(parts))
            intro = "La sÃ©lection des navires types couvre les unitÃ©s attendues au port. Ã‰chantillon :"
            return "\n".join([intro] + lignes)

        # Fallback minimal si aucune donnÃ©e
        return "La sÃ©lection des navires types reprÃ©sente les diffÃ©rentes catÃ©gories appelÃ©es Ã  frÃ©quenter le port."
    
    def _generate_remorqueurs(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("remorqueurs", rapport_data)
        return self._generate_with_ai("remorqueurs", prompt) or \
            "Les moyens d'assistance comprennent des remorqueurs adaptÃ©s aux navires Ã©tudiÃ©s."
    
    def _generate_simulations(self, rapport_data: dict) -> str:
        simulations = rapport_data.get("simulations", {})
        nb = len(simulations.get("simulations", []))
        prompt = self._build_prompt("simulations", rapport_data)
        result = self._generate_with_ai("simulations", prompt)
        return result or f"La mÃ©thodologie repose sur {nb} simulations numÃ©riques reprÃ©sentatives."
    
    def _generate_scenarios_urgence(self, rapport_data: dict) -> str:
        prompt = self._build_prompt("scenarios_urgence", rapport_data)
        return self._generate_with_ai("scenarios_urgence", prompt) or \
            "Les scÃ©narios d'urgence testent la rÃ©action en cas de dÃ©faillance des Ã©quipements."
    
    # =========================================================================
    # FALLBACKS
    # =========================================================================
    
    def _generate_fallback_sections(self, rapport_data: dict) -> dict:
        """GÃ©nÃ¨re toutes les sections en mode fallback"""
        print("âš ï¸ Mode fallback complet")
        
        sections = {}
        for section_name in self.section_generators.keys():
            try:
                sections[section_name] = self._get_fallback_for_section(section_name, rapport_data)
            except:
                pass
        
        return sections
    
    def _get_fallback_for_section(self, section_name: str, rapport_data: dict) -> str:
        """Retourne le fallback pour une section"""
        fallbacks = {
            "introduction": self._fallback_introduction(rapport_data),
            "analyse": self._fallback_analyse(rapport_data),
            "donnees_entree_intro": "Cette section prÃ©sente les donnÃ©es d'entrÃ©e de l'Ã©tude.",
            "donnees_entree_conditions_intro": "Les conditions environnementales influencent directement la manÅ“uvrabilitÃ© des navires.",
            "donnees_entree_bathymetrie": "La bathymÃ©trie a Ã©tÃ© Ã©tablie Ã  partir de relevÃ©s rÃ©cents.",
            "donnees_entree_houle": "Les conditions de houle correspondent aux Ã©tats de mer caractÃ©ristiques.",
            "donnees_entree_vent": "Les donnÃ©es de vent reprÃ©sentent les conditions mÃ©tÃ©orologiques locales.",
            "donnees_entree_courant": "Les conditions de courant intÃ¨grent les effets de marÃ©e.",
            "donnees_entree_maree": "Le rÃ©gime de marÃ©e dÃ©finit les variations du niveau d'eau.",
            "donnees_entree_agitation": "L'agitation rÃ©siduelle influence les conditions de manÅ“uvre.",
            "analyse_stats": "Statistiques des simulations : taux de rÃ©ussite global, rÃ©partition par manÅ“uvre et par navire, conditions critiques.",
            "analyse_recommandations": "Recommandations : seuils mÃ©tÃ©o, configuration remorqueurs, points de vigilance sur les scÃ©narios critiques.",
            "navires": "La sÃ©lection des navires reprÃ©sente les catÃ©gories appelÃ©es Ã  frÃ©quenter le port.",
            "remorqueurs": "Les moyens d'assistance comprennent des remorqueurs adaptÃ©s.",
            "simulations": "La mÃ©thodologie repose sur une approche numÃ©rique temps rÃ©el.",
            "scenarios_urgence": "Les scÃ©narios d'urgence Ã©valuent la rÃ©action en cas de dÃ©faillance.",
        }
        
        meta = rapport_data.get("metadonnees", {})
        titre = meta.get("titre", "Ã‰tude")
        
        return fallbacks.get(section_name, f"Section {section_name} de l'Ã©tude {titre}.")
    
    def _fallback_introduction(self, rapport_data: dict) -> str:
        meta = rapport_data.get("metadonnees", {})
        titre = meta.get("titre", "Ã‰tude de manÅ“uvrabilitÃ©")
        client = meta.get("client", "l'AutoritÃ© Portuaire")
        
        return f"""L'Ã©tude de manÅ“uvrabilitÃ© intitulÃ©e "{titre}" a Ã©tÃ© rÃ©alisÃ©e pour {client}. Cette Ã©tude vise Ã  Ã©valuer les capacitÃ©s de manÅ“uvre des navires dans les configurations portuaires Ã©tudiÃ©es.

Les objectifs principaux comprennent l'Ã©valuation de la faisabilitÃ© des manÅ“uvres d'accostage et d'appareillage, l'identification des conditions environnementales critiques, et la dÃ©finition des procÃ©dures d'assistance optimales.

La mÃ©thodologie employÃ©e s'appuie sur la modÃ©lisation numÃ©rique et la simulation de manÅ“uvres reprÃ©sentatives des conditions d'exploitation futures."""
    
    def _fallback_analyse(self, rapport_data: dict) -> str:
        simulations = rapport_data.get("simulations", {}).get("simulations", [])
        nb = len(simulations)
        
        if simulations:
            nb_ok = sum(1 for s in simulations if s.get("resultat") == "RÃ©ussite")
            taux = round((nb_ok / nb) * 100, 1) if nb > 0 else 0
            
            return f"""L'analyse des {nb} simulations rÃ©vÃ¨le un taux de rÃ©ussite de {taux}%. Les configurations testÃ©es permettent d'Ã©valuer la faisabilitÃ© des manÅ“uvres dans diverses conditions.

Les rÃ©sultats montrent l'importance de l'assistance par remorqueurs pour garantir la sÃ©curitÃ© des opÃ©rations, particuliÃ¨rement lors de conditions mÃ©tÃ©orologiques dÃ©gradÃ©es."""
        
        return """L'analyse des simulations rÃ©vÃ¨le les performances gÃ©nÃ©rales du systÃ¨me portuaire Ã©tudiÃ©. Les rÃ©sultats dÃ©montrent l'importance de l'assistance au pilotage et de la prise en compte des conditions environnementales."""
    
    def _print_stats(self, stats: dict, total: int):
        """Affiche les statistiques"""
        self._log(f"\nğŸ“Š Statistiques:")
        self._log(f"  âœ… SuccÃ¨s: {stats['success']}/{total}")
        self._log(f"  âš ï¸ Fallback: {stats['fallback']}/{total}")
        self._log(f"  âŒ Ã‰checs: {stats['failed']}/{total}")
    
    def get_system_status(self) -> Dict[str, Any]:
        """Retourne le statut du systÃ¨me"""
        status = {
            "overall_readiness": 0,
            "training_available": False,
            "api_available": REQUESTS_AVAILABLE,
            "sections_count": len(self.section_generators),
            "model": self.default_model,
            "recommendation": "",
        }
        
        if self.load_training_data():
            status["training_available"] = True
            status["overall_readiness"] = 70
            
            if REQUESTS_AVAILABLE:
                status["overall_readiness"] = 90
                status["recommendation"] = "âœ… SystÃ¨me opÃ©rationnel"
            else:
                status["recommendation"] = "âš ï¸ API non disponible - Mode fallback"
        else:
            status["recommendation"] = "âŒ EntraÃ®nement requis"
        
        return status

    # =========================================================================
    # LOGGING UTIL
    # =========================================================================
    def _log(self, message: str):
        if self.verbose:
            print(message)


# =============================================================================
# INTERFACES PUBLIQUES
# =============================================================================

def create_pretrained_manager() -> PretrainedManager:
    """CrÃ©e un gestionnaire"""
    return PretrainedManager()


def generate_granular_sections_for_report(rapport_data: dict, 
                                          sections_requested: List[str] = None,
                                          progress_mgr=None,
                                          progress_range=None) -> dict:
    """
    Interface principale - GÃ©nÃ¨re les sections avec agents prÃ©-entraÃ®nÃ©s
    
    Args:
        rapport_data: DonnÃ©es du rapport
        sections_requested: Sections spÃ©cifiques (None = toutes)
        
    Returns:
        Dict {section_name: content}
    """
    try:
        manager = create_pretrained_manager()
        if progress_mgr:
            progress_mgr.log_detail("DÃ©marrage de la gÃ©nÃ©ration IA (agents prÃ©-entraÃ®nÃ©s)")
        # Injection du progress_mgr et de la plage de progression si supportÃ©
        try:
            result = manager.generate_granular_sections_for_report(
                rapport_data, sections_requested,
                progress_mgr=progress_mgr,
                progress_range=progress_range
            )
        except TypeError:
            # fallback signature sans progress_mgr/progress_range
            result = manager.generate_granular_sections_for_report(rapport_data, sections_requested)
        if progress_mgr:
            progress_mgr.log_detail("Sections IA gÃ©nÃ©rÃ©es (agents prÃ©-entraÃ®nÃ©s)")
        return result
    except Exception as e:
        print(f"âŒ Erreur: {e}")
        return {}


def get_granular_system_status() -> Dict[str, Any]:
    """Retourne le statut du systÃ¨me"""
    try:
        manager = create_pretrained_manager()
        return manager.get_system_status()
    except Exception as e:
        return {"error": str(e), "overall_readiness": 0}
    
