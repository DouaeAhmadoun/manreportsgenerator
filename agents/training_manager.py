import os
import json
import docx
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
from jinja2 import Environment, FileSystemLoader, Template

try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False


class TrainingManager:
    """Gestionnaire d'entra√Ænement granulaire pour TOUTES les sections"""
    
    def __init__(self, 
                 reports_directory: str = "examples",
                 cache_directory: str = "agents/training_cache",
                 prompts_directory: str = "prompts"):
        
        self.reports_directory = Path(reports_directory)
        self.cache_directory = Path(cache_directory)
        self.prompts_directory = Path(prompts_directory)
        
        # Cr√©er les dossiers
        self.cache_directory.mkdir(parents=True, exist_ok=True)
        self.reports_directory.mkdir(exist_ok=True)
        
        # Fichiers de cache
        self.extracted_cache = self.cache_directory / "extracted_examples_granular.json"
        self.training_cache = self.cache_directory / "training_data_granular.json"
        self.metadata_cache = self.cache_directory / "training_metadata_granular.json"
        self.prompts_cache = self.cache_directory / "prompts_mapping_granular.json"
         
        # Configuration Jinja2
        self.jinja_env = Environment(
            loader=FileSystemLoader(str(self.prompts_directory)),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # üéØ MAPPING GRANULAIRE COMPLET - Toutes les sections/sous-sections
        self.granular_section_mapping = {
            # Sections principales
            "introduction": "introduction.txt",
            "analyse": "analyse.txt", 
            "conclusion": "conclusion.txt",
            
            # Donn√©es d'entr√©e - GRANULAIRE
            "donnees_entree_intro": "donnees_entree_intro.txt",
            "donnees_entree_plan_masse": "donnees_entree_plan_masse.txt",
            "donnees_entree_bathymetrie": "donnees_entree_bathymetrie.txt",
            "donnees_entree_balisage": "donnees_entree_balisage.txt",
            "donnees_entree_conditions_intro": "donnees_entree_conditions_intro.txt",
            "donnees_entree_houle": "donnees_entree_houle.txt",
            "donnees_entree_vent": "donnees_entree_vent.txt",
            "donnees_entree_courant": "donnees_entree_courant.txt",
            "donnees_entree_maree": "donnees_entree_maree.txt",
            "donnees_entree_agitation": "donnees_entree_agitation.txt",
            "donnees_entree_synthese": "donnees_entree_synthese.txt",
            
            # Navires - GRANULAIRE
            "navires": "navires.txt",
            "remorqueurs": "remorqueurs.txt",
            
            # Simulations - GRANULAIRE
            "simulations": "simulations.txt",
            "scenarios_urgence": "scenarios_urgence.txt"
        }
        
        # üîç PATTERNS D'EXTRACTION GRANULAIRES
        self.granular_extraction_patterns = {
            # Introduction
            "introduction": [
                r"(?i)(?:1\.?\s*)?introduction.*?(?=(?:\n\s*\d+\.|\n\s*[IVX]+\.|\Z))",
                r"(?i)(?:contexte|pr√©sentation|objectif).*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            
            # Donn√©es d'entr√©e - GRANULAIRE
            "donnees_entree_plan_masse": [
                r"(?i)plan\s+de\s+masse.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)am√©nagement.*?portuaire.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "donnees_entree_bathymetrie": [
                r"(?i)bathym[√©√®]trie.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)profondeur.*?fond.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "donnees_entree_balisage": [
                r"(?i)balisage.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)signalisation.*?maritime.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "donnees_entree_houle": [
                r"(?i)houle.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)vagues.*?hauteur.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "donnees_entree_vent": [
                r"(?i)vent.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)force.*?direction.*?vent.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "donnees_entree_courant": [
                r"(?i)courant.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)vitesse.*?courant.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "donnees_entree_maree": [
                r"(?i)mar[√©e]e.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)niveau.*?eau.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "donnees_entree_agitation": [
                r"(?i)agitation.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)oscillation.*?port.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            
            # Navires - GRANULAIRE
            "navires": [
                r"(?i)(?:navires?\s+[√†a]\s+tester|s[√©e]lection.*?navires?).*?(?=(?:\n\s*\d+\.|\Z))",
                r"(?i)caract[√©e]ristiques.*?navires?.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "remorqueurs": [
                r"(?i)remorqueurs?.*?(?=(?:\n\s*\d+\.|\n\s*[A-Z]\.|\Z))",
                r"(?i)assistance.*?pilotage.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            
            # Simulations - GRANULAIRE
            "simulations": [
                r"(?i)(?:essais\s+r[√©e]alis[√©e]s|simulations?).*?(?=(?:\n\s*\d+\.|\Z))",
                r"(?i)m[√©e]thodologie.*?simulation.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            "scenarios_urgence": [
                r"(?i)(?:sc[√©e]narios?\s+d['']urgence|situations?\s+d['']urgence).*?(?=(?:\n\s*\d+\.|\Z))",
                r"(?i)proc[√©e]dures?\s+d['']urgence.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            
            # Analyse - GRANULAIRE
            "analyse": [
                r"(?i)(?:4\.?\s*)?analyse.*?r[√©e]sultats.*?(?=(?:\n\s*\d+\.|\Z))",
                r"(?i)statistiques.*?g[√©e]n[√©e]rales.*?(?=(?:\n\s*\d+\.|\Z))"
            ],
            
            # Conclusion
            "conclusion": [
                r"(?i)(?:5\.?\s*)?conclusion.*?(?=(?:\n\s*\d+\.|\Z))",
                r"(?i)recommandations.*?(?=(?:\n\s*\d+\.|\Z))"
            ]
        }
        
        print(f"üéØ TrainingManager initialis√©")
        print(f"üîç Sections granulaires: {len(self.granular_section_mapping)}")
    
    def force_retrain_granular(self) -> Dict[str, Any]:
        """Force le r√©-entra√Ænement granulaire complet"""
        
        print("\n" + "="*70)
        print("üî• FOR√áAGE DU R√â-ENTRA√éNEMENT GRANULAIRE")
        print("="*70)
        
        # 1. Analyser les prompts disponibles
        print("\nüìù PHASE 1: Analyse des prompts granulaires")
        prompts_analysis = self._analyze_granular_prompts()
        
        # 2. Supprimer le cache existant
        print("\nüßπ PHASE 2: Suppression du cache")
        self._clear_cache()
        
        # 3. Extraire TOUTES les sections granulaires
        print("\nüìö PHASE 3: Extraction granulaire compl√®te")
        extracted_data = self._extract_granular_examples()
        
        if not extracted_data or not any(extracted_data.values()):
            print("‚ùå Aucun exemple extrait")
            return {"status": "failed", "reason": "no_examples"}
        
        # 4. Traiter avec int√©gration prompts granulaires
        print("\nüîß PHASE 4: Traitement granulaire avec prompts")
        training_data = self._process_granular_training_data(extracted_data, prompts_analysis)
        
        # 5. Sauvegarder
        print("\nüíæ PHASE 5: Sauvegarde granulaire")
        self._save_granular_cache(extracted_data, training_data, prompts_analysis)
        
        # 6. G√©n√©rer m√©tadonn√©es
        metadata = self._generate_granular_metadata(training_data, prompts_analysis)
        
        print("\n‚úÖ R√â-ENTRA√éNEMENT GRANULAIRE TERMIN√â")
        self._print_granular_summary(metadata)
        
        return metadata
    
    def _analyze_granular_prompts(self) -> Dict[str, Any]:
        """Analyse TOUS les prompts granulaires disponibles"""
        
        analysis = {
            "available_prompts": {},
            "missing_prompts": [],
            "total_prompts": 0,
            "sections_covered": [],
            "granular_coverage": {
                "donnees_entree": 0,
                "navires": 0,
                "simulations": 0,
                "analyse": 0,
                "general": 0
            }
        }
        
        print(f"üîç Analyse granulaire du dossier: {self.prompts_directory}")
        
        if not self.prompts_directory.exists():
            print(f"‚ùå Dossier prompts non trouv√©: {self.prompts_directory}")
            return analysis
        
        # V√©rifier chaque mapping granulaire
        for section_name, prompt_file in self.granular_section_mapping.items():
            prompt_path = self.prompts_directory / prompt_file
            
            if prompt_path.exists():
                try:
                    with open(prompt_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                    
                    if content:
                        analysis["available_prompts"][section_name] = {
                            "file": prompt_file,
                            "path": str(prompt_path),
                            "content": content,
                            "length": len(content),
                            "variables": self._extract_jinja_variables(content),
                            "has_conditions": "{% if" in content,
                            "has_loops": "{% for" in content,
                            "category": self._categorize_section(section_name)
                        }
                        
                        # Compter par cat√©gorie
                        category = self._categorize_section(section_name)
                        if category in analysis["granular_coverage"]:
                            analysis["granular_coverage"][category] += 1
                        
                        print(f"  ‚úÖ {section_name}: {prompt_file} ({len(content)} chars)")
                        
                        # Analyser les sections principales
                        if section_name in ["introduction", "analyse", "conclusion"]:
                            analysis["sections_covered"].append(section_name)
                    else:
                        print(f"  ‚ö†Ô∏è {section_name}: {prompt_file} (vide)")
                        analysis["missing_prompts"].append(section_name)
                
                except Exception as e:
                    print(f"  ‚ùå {section_name}: {prompt_file} (erreur: {e})")
                    analysis["missing_prompts"].append(section_name)
            else:
                print(f"  ‚ùå {section_name}: {prompt_file} (non trouv√©)")
                analysis["missing_prompts"].append(section_name)
        
        analysis["total_prompts"] = len(analysis["available_prompts"])
        
        print(f"\nüìä R√©sultats granulaires:")
        print(f"  ‚úÖ Prompts trouv√©s: {analysis['total_prompts']}")
        print(f"  ‚ùå Prompts manquants: {len(analysis['missing_prompts'])}")
        print(f"  üéØ Sections principales: {len(analysis['sections_covered'])}/3")
        print(f"  üìä Donn√©es d'entr√©e: {analysis['granular_coverage']['donnees_entree']}")
        print(f"  üö¢ Navires: {analysis['granular_coverage']['navires']}")
        print(f"  üåÄ Simulations: {analysis['granular_coverage']['simulations']}")
        print(f"  üìà Analyse: {analysis['granular_coverage']['analyse']}")
        
        return analysis
    
    def _categorize_section(self, section_name: str) -> str:
        """Cat√©gorise une section pour les statistiques"""
        if section_name.startswith("donnees_entree"):
            return "donnees_entree"
        elif section_name.startswith("navires") or section_name.startswith("remorqueurs"):
            return "navires"
        elif section_name.startswith("simulations") or section_name.startswith("scenarios"):
            return "simulations"
        elif section_name.startswith("analyse"):
            return "analyse"
        else:
            return "general"
    
    def _extract_granular_examples(self) -> Dict[str, List[Dict]]:
        """Extrait TOUS les exemples granulaires de tous les rapports"""
        
        if not self.reports_directory.exists():
            print(f"‚ùå Dossier {self.reports_directory} non trouv√©")
            return {}
        
        all_files = []
        for ext in ['.docx', '.pdf']:
            all_files.extend(list(self.reports_directory.glob(f"*{ext}")))
        
        if not all_files:
            print(f"‚ùå Aucun fichier dans {self.reports_directory}")
            return {}
        
        print(f"üìÑ {len(all_files)} fichiers trouv√©s pour extraction granulaire")
        
        # Initialiser les conteneurs pour TOUTES les sections granulaires
        extracted_examples = {}
        for section_name in self.granular_section_mapping.keys():
            extracted_examples[section_name] = []
        
        processed_count = 0
        for file_path in all_files:
            try:
                print(f"üîÑ Traitement granulaire: {file_path.name}")
                
                # Extraction selon le type de fichier
                if file_path.suffix.lower() == '.pdf':
                    sections = self._extract_granular_sections_from_pdf(file_path)
                else:
                    sections = self._extract_granular_sections_from_docx(file_path)
                
                # Traiter TOUTES les sections trouv√©es
                for section_name, content in sections.items():
                    if content and section_name in extracted_examples:
                        metadata = self._analyze_granular_content_metadata(content, section_name)
                        quality_score = self._calculate_granular_quality_score(content, section_name)
                        
                        if quality_score >= 0.2:  # Seuil plus bas pour sections granulaires
                            example = {
                                "source_file": file_path.name,
                                "content": content,
                                "metadata": metadata,
                                "quality_score": quality_score,
                                "word_count": len(content.split()),
                                "section_category": self._categorize_section(section_name),
                                "extraction_date": datetime.now().isoformat()
                            }
                            extracted_examples[section_name].append(example)
                            print(f"    ‚úÖ {section_name}: {len(content)} chars (Q: {quality_score:.2f})")
                        else:
                            print(f"    ‚ö†Ô∏è {section_name}: qualit√© trop faible ({quality_score:.2f})")
                
                processed_count += 1
                
            except Exception as e:
                print(f"‚ùå Erreur granulaire: {e}")
        
        print(f"\nüìä Extraction granulaire termin√©e: {processed_count} rapports trait√©s")
        
        # Afficher r√©sum√© granulaire
        total_examples = 0
        for section_name, examples in extracted_examples.items():
            count = len(examples)
            total_examples += count
            if count > 0:
                avg_quality = sum(ex["quality_score"] for ex in examples) / count
                category = self._categorize_section(section_name)
                print(f"  üìã {section_name} ({category}): {count} exemples (Q: {avg_quality:.2f})")
        
        print(f"\nüéØ Total granulaire: {total_examples} exemples dans {len([s for s in extracted_examples.values() if s])} sections")
        
        return extracted_examples
    
    def _extract_granular_sections_from_docx(self, docx_path: Path) -> Dict[str, str]:
        """Extrait TOUTES les sections granulaires d'un rapport Word"""
        try:
            doc = docx.Document(str(docx_path))
            full_text = '\n'.join([p.text for p in doc.paragraphs if p.text.strip()])
            
            sections = {}
            for section_name, patterns in self.granular_extraction_patterns.items():
                content = self._extract_section_content(full_text, patterns)
                if content:
                    sections[section_name] = content
            
            return sections
            
        except Exception as e:
            print(f"‚ùå Erreur lecture granulaire {docx_path}: {e}")
            return {}
    
    def _extract_granular_sections_from_pdf(self, pdf_path: Path) -> Dict[str, str]:
        """Extrait TOUTES les sections granulaires d'un PDF"""
        
        if not PDF_AVAILABLE:
            print(f"‚ö†Ô∏è PyPDF2 non install√© - PDF ignor√©: {pdf_path.name}")
            return {}
        
        try:
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
            
            if len(text.strip()) < 100:
                print(f"    ‚ö†Ô∏è PDF {pdf_path.name}: texte insuffisant")
                return {}
            
            # Nettoyer le texte PDF
            text = self._clean_pdf_text(text)
            
            # Extraire toutes les sections granulaires
            sections = {}
            for section_name, patterns in self.granular_extraction_patterns.items():
                content = self._extract_section_content(text, patterns)
                if content:
                    sections[section_name] = content
            
            return sections
            
        except Exception as e:
            print(f"‚ùå Erreur extraction PDF granulaire {pdf_path.name}: {e}")
            return {}
    
    def _analyze_granular_content_metadata(self, content: str, section_name: str) -> Dict[str, Any]:
        """Analyse les m√©tadonn√©es granulaires du contenu"""
        
        metadata = {
            "length_words": len(content.split()),
            "length_chars": len(content),
            "section_category": self._categorize_section(section_name),
            "technical_density": 0,
            "has_quantitative_data": False,
            "specific_mentions": {}
        }
        
        content_lower = content.lower()
        
        # Mentions sp√©cifiques selon la cat√©gorie
        if section_name.startswith("donnees_entree"):
            metadata["specific_mentions"] = {
                "profondeur": content_lower.count("profondeur") + content_lower.count("fond"),
                "metres": len(re.findall(r'\d+[\s]*m(?:√®tres?)?', content)),
                "conditions": content_lower.count("condition"),
                "donnees": content_lower.count("donn√©es") + content_lower.count("donnees")
            }
        elif section_name.startswith("navires"):
            metadata["specific_mentions"] = {
                "navires": content_lower.count("navire"),
                "longueur": content_lower.count("longueur"),
                "tirant_eau": content_lower.count("tirant"),
                "caracteristiques": content_lower.count("caract√©ristique")
            }
        elif section_name.startswith("simulations"):
            metadata["specific_mentions"] = {
                "simulations": content_lower.count("simulation"),
                "essais": content_lower.count("essai"),
                "scenarios": content_lower.count("sc√©nario"),
                "tests": content_lower.count("test")
            }
        elif section_name.startswith("analyse"):
            metadata["specific_mentions"] = {
                "resultats": content_lower.count("r√©sultat"),
                "performance": content_lower.count("performance"),
                "reussite": content_lower.count("r√©ussite"),
                "echec": content_lower.count("√©chec")
            }
        
        # Densit√© technique g√©n√©rale
        total_mentions = sum(metadata["specific_mentions"].values())
        if len(content.split()) > 0:
            metadata["technical_density"] = total_mentions / len(content.split())
        
        # Donn√©es quantitatives
        metadata["has_quantitative_data"] = bool(re.search(r'\d+[%\s]*(?:kts?|m|km|¬∞|n≈ìuds|Hz|s)', content))
        
        return metadata
    
    def _calculate_granular_quality_score(self, content: str, section_name: str) -> float:
        """Calcule un score de qualit√© granulaire adapt√© √† chaque type de section"""
        
        if not content:
            return 0.0
        
        score = 0.0
        word_count = len(content.split())
        metadata = self._analyze_granular_content_metadata(content, section_name)
        
        # Plages optimales adapt√©es par cat√©gorie
        optimal_ranges = {
            "introduction": (200, 600),
            "analyse": (300, 1000),
            "conclusion": (150, 500),
            "donnees_entree_intro": (100, 300),
            "donnees_entree_plan_masse": (50, 200),
            "donnees_entree_bathymetrie": (50, 200),
            "donnees_entree_houle": (50, 150),
            "donnees_entree_vent": (50, 150),
            "donnees_entree_courant": (50, 150),
            "donnees_entree_maree": (50, 150),
            "navires": (100, 400),
            "remorqueurs": (50, 200),
            "simulations": (150, 500),
            "scenarios_urgence": (100, 300)
        }
        
        # Plage par d√©faut pour sections non sp√©cifi√©es
        min_words, max_words = optimal_ranges.get(section_name, (50, 300))
        
        # Score bas√© sur la longueur
        if min_words <= word_count <= max_words:
            score += 0.4
        elif word_count >= min_words * 0.5:
            score += 0.2
        
        # Score bas√© sur la densit√© technique
        tech_density = metadata["technical_density"]
        if tech_density > 0.05:
            score += 0.3
        elif tech_density > 0.02:
            score += 0.2
        elif tech_density > 0.01:
            score += 0.1
        
        # Score bas√© sur la structure
        sentence_count = content.count('.')
        if sentence_count >= 2:
            score += 0.2
        elif sentence_count >= 1:
            score += 0.1
        
        # Bonus pour donn√©es quantitatives
        if metadata["has_quantitative_data"]:
            score += 0.1
        
        return min(score, 1.0)
    
    def _process_granular_training_data(self, extracted_data: Dict, prompts_analysis: Dict) -> Dict[str, List[Dict]]:
        """Traite les donn√©es granulaires avec int√©gration des prompts"""
        
        processed_data = {}
        
        for section_name, examples in extracted_data.items():
            if not examples:
                processed_data[section_name] = []
                continue
            
            print(f"üîß Traitement granulaire {section_name}: {len(examples)} exemples")
            
            # Trier par qualit√©
            examples.sort(key=lambda x: x["quality_score"], reverse=True)
            
            # S√©lectionner les meilleurs (max 3 par section granulaire)
            max_examples = 20 if self._categorize_section(section_name) != "general" else 30
            best_examples = examples[:max_examples]
            
            # Enrichir avec les prompts granulaires
            for example in best_examples:
                # Ajouter les informations de prompt
                if section_name in prompts_analysis["available_prompts"]:
                    prompt_info = prompts_analysis["available_prompts"][section_name]
                    example["prompt_template"] = prompt_info["content"]
                    example["prompt_variables"] = prompt_info["variables"]
                    example["prompt_file"] = prompt_info["file"]
                    example["has_custom_prompt"] = True
                else:
                    example["has_custom_prompt"] = False
                    example["prompt_template"] = self._generate_granular_fallback_prompt(section_name)
                
                # G√©n√©rer des prompts d'entra√Ænement contextualis√©s granulaires
                example["training_prompt"] = self._generate_granular_contextual_training_prompt(example, section_name)
                example["training_weight"] = self._calculate_granular_training_weight(example)
            
            processed_data[section_name] = best_examples
            print(f"  ‚úÖ {len(best_examples)} exemples granulaires enrichis")
        
        return processed_data
    
    def _generate_granular_contextual_training_prompt(self, example: Dict, section_name: str) -> str:
        """G√©n√®re un prompt d'entra√Ænement contextualis√© granulaire"""
        
        if example.get("has_custom_prompt"):
            # Utiliser le vrai prompt avec des donn√©es mock√©es granulaires
            prompt_template = example["prompt_template"]
            
            # Cr√©er un contexte mock granulaire
            mock_context = self._create_granular_mock_context(example, section_name)
            
            try:
                template = Template(prompt_template)
                rendered_prompt = template.render(**mock_context)
                return rendered_prompt
            except Exception as e:
                print(f"    ‚ö†Ô∏è Erreur rendu prompt granulaire {section_name}: {e}")
                return self._generate_granular_fallback_prompt(section_name)
        else:
            return self._generate_granular_fallback_prompt(section_name)
    
    def _create_granular_mock_context(self, example: Dict, section_name: str) -> Dict:
        """Cr√©e un contexte mock granulaire adapt√© √† chaque section"""
        
        metadata = example.get("metadata", {})
        category = self._categorize_section(section_name)
        
        # Contexte de base commun
        base_context = {
            "metadonnees": {
                "titre": "√âtude de Man≈ìuvrabilit√© - Terminal Conteneurs",
                "client": "Autorit√© Portuaire",
                "date": datetime.now().strftime("%Y-%m-%d"),
                "port": "Port de Tanger Med"
            }
        }
        
        # Contexte sp√©cifique par cat√©gorie
        if category == "donnees_entree":
            base_context.update({
                "donnees_entree": {
                    "bathymetrie": {
                        "source": "Relev√© bathym√©trique 2024",
                        "profondeur_minimale": "12.5m",
                        "profondeur_maximale": "18.0m",
                        "commentaire": "Bathym√©trie adapt√©e aux navires de grande taille"
                    },
                    "conditions_environnementales": {
                        "houle": {
                            "valeurs_retenues": "Hs = 2.5m, Tp = 8s",
                            "direction": "Nord-Ouest",
                            "commentaire": "Conditions de houle mod√©r√©es"
                        },
                        "vent": {
                            "valeurs_retenues": "30 kts, rafales 40 kts",
                            "direction": "Ouest",
                            "commentaire": "Vent dominant d'ouest"
                        },
                        "courant": {
                            "valeurs_retenues": "1.5 kts",
                            "direction": "Est-Nord-Est",
                            "commentaire": "Courant de mar√©e"
                        },
                        "maree": {
                            "valeurs_retenues": "Marnage 4.2m",
                            "type": "Semi-diurne",
                            "commentaire": "Mar√©e atlantique"
                        },
                        "agitation": {
                            "valeurs_retenues": "0.5m √† 0.8m",
                            "periode": "6-12s",
                            "commentaire": "Agitation r√©siduelle dans le port"
                        }
                    },
                    "plan_de_masse": {
                        "phases": [
                            {"nom": "Phase 1", "description": "Configuration actuelle"},
                            {"nom": "Phase 2", "description": "Extension terminale"}
                        ]
                    }
                }
            })
        
        elif category == "navires":
            base_context.update({
                "navires_liste": [
                    {
                        "nom": "Cargo 1200 EVP",
                        "type": "Porte-conteneurs",
                        "longueur": "210m",
                        "largeur": "32m",
                        "tirant_eau_av": "9.5m",
                        "tirant_eau_ar": "10.2m"
                    },
                    {
                        "nom": "Cargo 800 EVP",
                        "type": "Porte-conteneurs",
                        "longueur": "180m",
                        "largeur": "28m",
                        "tirant_eau_av": "8.5m",
                        "tirant_eau_ar": "9.0m"
                    }
                ],
                "remorqueurs_liste": [
                    {
                        "nom": "Remorqueur RT-01",
                        "type": "Azimuth",
                        "longueur": "28m",
                        "largeur": "12m",
                        "puissance": "3000 kW"
                    }
                ]
            })
        
        elif category == "simulations":
            base_context.update({
                "simulations": {
                    "simulations": [
                        {
                            "numero_essai_original": 1,
                            "navire": "Cargo 1200 EVP",
                            "condition": "Normale",
                            "resultat": "R√©ussite",
                            "manoeuvre": "Accostage",
                            "remorqueurs": "2 remorqueurs",
                            "commentaire_pilote": "Man≈ìuvre r√©alis√©e sans difficult√©"
                        },
                        {
                            "numero_essai_original": 2,
                            "navire": "Cargo 800 EVP",
                            "condition": "Vent fort",
                            "resultat": "√âchec",
                            "manoeuvre": "Accostage",
                            "remorqueurs": "1 remorqueur",
                            "commentaire_pilote": "Assistance insuffisante par vent fort"
                        }
                    ]
                },
                "nb_simulations": 15,
                "simulations_description": "M√©thodologie bas√©e sur la simulation num√©rique temps r√©el"
            })
        
        elif category == "analyse":
            base_context.update({
                "analyse_synthese": {
                    "nombre_essais": 15,
                    "nombre_reussis": 12,
                    "nombre_echecs": 3,
                    "taux_reussite_pct": 80.0,
                    "nombre_scenarios_urgence": 3,
                    "conditions_critiques_liste": [
                        "Vent sup√©rieur √† 35 kts avec courant oppos√©",
                        "Combinaison houle + vent de travers",
                        "Visibilit√© r√©duite avec vent fort"
                    ],
                    "commentaire": "L'analyse r√©v√®le une bonne performance g√©n√©rale des man≈ìuvres"
                }
            })
        
        # Enrichir selon les mentions sp√©cifiques
        if section_name.endswith("_houle"):
            base_context["houle_conditions"] = True
        if section_name.endswith("_vent"):
            base_context["vent_conditions"] = True
        if section_name.endswith("_courant"):
            base_context["courant_conditions"] = True
        if section_name.endswith("_maree"):
            base_context["maree_conditions"] = True
        if section_name.endswith("_agitation"):
            base_context["agitation_conditions"] = True
        
        return base_context
    
    def _generate_granular_fallback_prompt(self, section_name: str) -> str:
        """G√©n√®re un prompt de fallback granulaire sp√©cifique"""
        
        fallback_prompts = {
            # Sections principales
            "introduction": "Tu es un ing√©nieur maritime expert. R√©dige l'introduction d'un rapport de man≈ìuvrabilit√© professionnel.",
            "analyse": "Tu es un expert en simulations. Analyse les r√©sultats des simulations de man≈ìuvrabilit√©.",
            "conclusion": "Tu es un ing√©nieur maritime. R√©dige la conclusion d'un rapport de man≈ìuvrabilit√©.",
            
            # Donn√©es d'entr√©e granulaires
            "donnees_entree_intro": "Pr√©sente les donn√©es d'entr√©e d'une √©tude de man≈ìuvrabilit√©.",
            "donnees_entree_plan_masse": "D√©cris le plan de masse et l'am√©nagement portuaire.",
            "donnees_entree_bathymetrie": "Pr√©sente les donn√©es bathym√©triques du site d'√©tude.",
            "donnees_entree_balisage": "D√©cris le plan de balisage et la signalisation maritime.",
            "donnees_entree_houle": "Pr√©sente les conditions de houle retenues pour l'√©tude.",
            "donnees_entree_vent": "D√©cris les conditions de vent consid√©r√©es.",
            "donnees_entree_courant": "Pr√©sente les donn√©es de courant utilis√©es.",
            "donnees_entree_maree": "D√©cris les conditions de mar√©e retenues.",
            "donnees_entree_agitation": "Pr√©sente l'agitation portuaire consid√©r√©e.",
            "donnees_entree_synthese": "Synth√©tise l'ensemble des donn√©es environnementales.",
            
            # Navires granulaires
            "navires": "Pr√©sente les navires s√©lectionn√©s pour l'√©tude de man≈ìuvrabilit√©.",
            "remorqueurs": "Pr√©sente les remorqueurs et moyens d'assistance.",
            
            # Simulations granulaires
            "simulations": "Pr√©sente la m√©thodologie des simulations de man≈ìuvrabilit√©.",
            "scenarios_urgence": "D√©cris les sc√©narios d'urgence √©tudi√©s."
        }
        
        return fallback_prompts.get(section_name, f"R√©dige la section {section_name} d'un rapport technique de man≈ìuvrabilit√©.")
    
    def _calculate_granular_training_weight(self, example: Dict) -> float:
        """Calcule un poids d'entra√Ænement granulaire"""
        
        base_weight = example["quality_score"]
        
        # Bonus si prompt custom disponible
        if example.get("has_custom_prompt"):
            base_weight += 0.15
        
        # Bonus pour la sp√©cificit√© de la section
        category = example.get("section_category", "general")
        category_bonus = {
            "donnees_entree": 0.1,
            "navires": 0.1,
            "simulations": 0.15,
            "analyse": 0.15,
            "general": 0.05
        }.get(category, 0)
        
        # Bonus pour la densit√© technique
        metadata = example.get("metadata", {})
        tech_bonus = min(metadata.get("technical_density", 0) * 2, 0.2)
        
        # Bonus quantitatif
        quantitative_bonus = 0.1 if metadata.get("has_quantitative_data") else 0
        
        return min(base_weight + category_bonus + tech_bonus + quantitative_bonus, 1.0)
    
    def _save_granular_cache(self, extracted_data: Dict, training_data: Dict, prompts_analysis: Dict):
        """Sauvegarde le cache granulaire enrichi"""
        
        # Cache des exemples extraits granulaires
        with open(self.extracted_cache, 'w', encoding='utf-8') as f:
            json.dump(extracted_data, f, ensure_ascii=False, indent=2)
        
        # Cache des donn√©es d'entra√Ænement granulaires
        with open(self.training_cache, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        # Cache du mapping prompts granulaires
        with open(self.prompts_cache, 'w', encoding='utf-8') as f:
            json.dump(prompts_analysis, f, ensure_ascii=False, indent=2)
        
        print("‚úÖ Cache granulaire enrichi sauvegard√©")
    
    def _generate_granular_metadata(self, training_data: Dict, prompts_analysis: Dict) -> Dict[str, Any]:
        """G√©n√®re des m√©tadonn√©es granulaires enrichies"""
        
        metadata = {
            "training_date": datetime.now().isoformat(),
            "training_type": "granular_v2",
            "forced_retrain": True,
            "prompts_integration": True,
            "reports_directory": str(self.reports_directory),
            "prompts_directory": str(self.prompts_directory),
            "total_examples": sum(len(examples) for examples in training_data.values()),
            "total_sections": len(self.granular_section_mapping),
            "prompts_analysis": prompts_analysis,
            "sections": {},
            "quality_stats": {},
            "granular_coverage": {
                "donnees_entree": 0,
                "navires": 0,
                "simulations": 0,
                "analyse": 0,
                "general": 0
            }
        }
        
        # Statistiques granulaires par section
        all_qualities = []
        for section_name, examples in training_data.items():
            category = self._categorize_section(section_name)
            
            if examples:
                qualities = [ex["quality_score"] for ex in examples]
                weights = [ex["training_weight"] for ex in examples]
                word_counts = [ex["word_count"] for ex in examples]
                custom_prompts = sum(1 for ex in examples if ex.get("has_custom_prompt"))
                
                metadata["sections"][section_name] = {
                    "count": len(examples),
                    "category": category,
                    "avg_quality": sum(qualities) / len(qualities),
                    "best_quality": max(qualities),
                    "avg_weight": sum(weights) / len(weights),
                    "avg_words": sum(word_counts) / len(word_counts),
                    "custom_prompts": custom_prompts,
                    "prompt_coverage": custom_prompts / len(examples),
                    "best_sources": [ex["source_file"] for ex in examples[:2]]
                }
                
                # Compter par cat√©gorie
                if category in metadata["granular_coverage"]:
                    metadata["granular_coverage"][category] += len(examples)
                
                all_qualities.extend(qualities)
            else:
                metadata["sections"][section_name] = {
                    "count": 0,
                    "category": category,
                    "avg_quality": 0,
                    "best_quality": 0,
                    "avg_weight": 0,
                    "avg_words": 0,
                    "custom_prompts": 0,
                    "prompt_coverage": 0,
                    "best_sources": []
                }
        
        # Statistiques globales granulaires
        if all_qualities:
            metadata["quality_stats"] = {
                "overall_avg": sum(all_qualities) / len(all_qualities),
                "overall_best": max(all_qualities),
                "sections_with_data": len([s for s in metadata["sections"].values() if s["count"] > 0]),
                "sections_with_prompts": len([s for s in metadata["sections"].values() if s["custom_prompts"] > 0]),
                "granular_completeness": len([s for s in metadata["sections"].values() if s["count"] > 0]) / len(metadata["sections"])
            }
        
        # Sauvegarder
        with open(self.metadata_cache, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        return metadata
    
    def _print_granular_summary(self, metadata: Dict[str, Any]):
        """Affiche un r√©sum√© granulaire enrichi"""
        
        print("\n" + "="*70)
        print("üéâ R√âSUM√â DU R√â-ENTRA√éNEMENT GRANULAIRE")
        print("="*70)
        
        prompts_analysis = metadata.get("prompts_analysis", {})
        quality_stats = metadata.get("quality_stats", {})
        sections_data = metadata.get("sections", {})
        granular_coverage = metadata.get("granular_coverage", {})
        
        print(f"üî• Type: ENTRA√éNEMENT GRANULAIRE FORC√â")
        print(f"üìù Prompts granulaires: {prompts_analysis.get('total_prompts', 0)}")
        print(f"üìö Total exemples: {metadata.get('total_examples', 0)}")
        print(f"üéØ Sections totales: {metadata.get('total_sections', 0)}")
        print(f"üèÜ Qualit√© moyenne: {quality_stats.get('overall_avg', 0):.2f}")
        print(f"üìä Compl√©tude granulaire: {quality_stats.get('granular_completeness', 0):.1%}")
        
        print(f"\nüìù Couverture Prompts Granulaires:")
        print(f"  ‚úÖ Prompts trouv√©s: {prompts_analysis.get('total_prompts', 0)}/{metadata.get('total_sections', 0)}")
        print(f"  ‚ùå Prompts manquants: {len(prompts_analysis.get('missing_prompts', []))}")
        print(f"  üéØ Sections principales: {len(prompts_analysis.get('sections_covered', []))}/3")
        
        print(f"\nüìä Couverture par Cat√©gorie:")
        for category, count in granular_coverage.items():
            if count > 0:
                print(f"  üìã {category.replace('_', ' ').title()}: {count} exemples")
        
        print(f"\nüîç Top Sections par Qualit√©:")
        # Trier les sections par qualit√© moyenne
        sorted_sections = sorted(
            [(name, stats) for name, stats in sections_data.items() if stats["count"] > 0],
            key=lambda x: x[1]["avg_quality"],
            reverse=True
        )
        
        for i, (section_name, stats) in enumerate(sorted_sections[:10]):
            count = stats["count"]
            quality = stats["avg_quality"]
            prompts = stats["custom_prompts"]
            coverage = stats["prompt_coverage"]
            
            print(f"  {i+1:2d}. {section_name:25}: {count} ex. (Q:{quality:.2f}, P:{coverage:.1%})")
        
        if prompts_analysis.get("missing_prompts"):
            print(f"\n‚ö†Ô∏è Prompts granulaires manquants (premiers 10):")
            for missing in prompts_analysis["missing_prompts"][:10]:
                print(f"  ‚Ä¢ {missing}")
        
        print(f"\nüíæ Cache granulaire: {self.cache_directory}")
        print(f"üìÖ Date: {metadata.get('training_date', 'N/A')}")
    
    # M√©thodes utilitaires h√©rit√©es et adapt√©es
    def _extract_jinja_variables(self, content: str) -> List[str]:
        """Extrait les variables Jinja2 d'un prompt"""
        variables = re.findall(r'\{\{\s*([^}]+)\s*\}\}', content)
        clean_vars = []
        for var in variables:
            clean_var = var.split('|')[0].split('.')[0].strip()
            if clean_var and clean_var not in clean_vars:
                clean_vars.append(clean_var)
        return clean_vars
    
    def _clear_cache(self):
        """Supprime tout le cache granulaire existant"""
        cache_files = [
            self.extracted_cache,
            self.training_cache, 
            self.metadata_cache,
            self.prompts_cache
        ]
        
        for cache_file in cache_files:
            if cache_file.exists():
                cache_file.unlink()
                print(f"üóëÔ∏è Supprim√©: {cache_file.name}")
        
        print("‚úÖ Cache granulaire nettoy√©")
    
    def _extract_section_content(self, full_text: str, patterns: List[str]) -> str:
        """Extrait le contenu d'une section avec plusieurs patterns"""
        for pattern in patterns:
            match = re.search(pattern, full_text, re.DOTALL)
            if match:
                content = match.group(0).strip()
                content = self._clean_extracted_content(content)
                
                if len(content) > 50 and len(content.split()) > 10:  # Seuil plus bas pour granulaire
                    return content
        
        return ""
    
    def _clean_extracted_content(self, content: str) -> str:
        """Nettoie le contenu extrait"""
        content = re.sub(r'^\d+\.?\s*[A-Za-z\s]*\n?', '', content).strip()
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = re.sub(r'[^\w\s\.\,\;\:\!\?\(\)\-\'\"\n]', ' ', content)
        return content.strip()
    
    def _clean_pdf_text(self, text: str) -> str:
        """Nettoie le texte extrait d'un PDF"""
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        text = re.sub(r'\n\s*\d+\s*\n', '\n', text)
        return text.strip()
    
    # Interfaces publiques
    def get_training_status(self) -> Dict[str, Any]:
        """Retourne le statut de l'entra√Ænement granulaire"""
        if not self._is_training_valid():
            return {
                "status": "not_trained",
                "message": "Aucun entra√Ænement granulaire trouv√©",
                "ready": False,
                "type": "granular_v2"
            }
        
        metadata = self._load_training_metadata()
        return {
            "status": "trained",
            "message": "Agents granulaires pr√©-entra√Æn√©s disponibles",
            "ready": True,
            "type": "granular_v2",
            "metadata": metadata
        }
    
    def _is_training_valid(self) -> bool:
        """V√©rifie si l'entra√Ænement granulaire en cache est valide"""
        cache_files = [self.extracted_cache, self.training_cache, self.metadata_cache]
        if not all(f.exists() for f in cache_files):
            return False
        
        try:
            metadata = self._load_training_metadata()
            return metadata.get("training_type") == "granular_v2"
        except Exception:
            return False
    
    def _load_training_metadata(self) -> Dict[str, Any]:
        """Charge les m√©tadonn√©es d'entra√Ænement granulaire"""
        try:
            with open(self.metadata_cache, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            return {}
    
    def load_granular_training_data(self) -> Optional[Dict[str, List[Dict]]]:
        """Charge les donn√©es d'entra√Ænement granulaires depuis le cache"""
        if not self.training_cache.exists():
            return None
        
        try:
            with open(self.training_cache, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ùå Erreur chargement cache granulaire: {e}")
            return None


# ============================================================================
# INTERFACES SIMPLIFI√âES POUR INT√âGRATION
# ============================================================================

def setup_granular_training(reports_directory="examples") -> TrainingManager:
    """Configure et lance l'entra√Ænement granulaire si n√©cessaire"""
    manager = TrainingManager(reports_directory)
    
    status = manager.get_training_status()
    if not status["ready"]:
        print("üéì Lancement de l'entra√Ænement granulaire initial...")
        manager.force_retrain_granular()
    else:
        print("‚úÖ Agents granulaires d√©j√† entra√Æn√©s et pr√™ts")
    
    return manager


def get_granular_pretrained_data() -> Optional[Dict[str, List[Dict]]]:
    """Interface rapide pour charger les donn√©es granulaires pr√©-entra√Æn√©es"""
    manager = TrainingManager()
    return manager.load_granular_training_data()


def force_granular_retrain(reports_dir: str = "examples", 
                          prompts_dir: str = "prompts") -> bool:
    """Interface simple pour forcer le r√©-entra√Ænement granulaire"""
    
    try:
        manager = TrainingManager(
            reports_directory=reports_dir,
            prompts_directory=prompts_dir
        )
        
        result = manager.force_retrain_granular()
        return result.get("status") != "failed"
        
    except Exception as e:
        print(f"‚ùå Erreur r√©-entra√Ænement granulaire: {e}")
        return False


def check_granular_prompts_integration() -> Dict[str, Any]:
    """V√©rifie l'√©tat de l'int√©gration granulaire avec les prompts"""
    
    try:
        manager = TrainingManager()
        prompts_analysis = manager._analyze_granular_prompts()
        
        return {
            "prompts_found": prompts_analysis["total_prompts"],
            "prompts_missing": len(prompts_analysis["missing_prompts"]),
            "sections_covered": len(prompts_analysis["sections_covered"]),
            "granular_coverage": prompts_analysis["granular_coverage"],
            "available_prompts": list(prompts_analysis["available_prompts"].keys()),
            "missing_prompts": prompts_analysis["missing_prompts"],
            "ready_for_training": prompts_analysis["total_prompts"] > 0,
            "completeness": prompts_analysis["total_prompts"] / len(manager.granular_section_mapping)
        }
        
    except Exception as e:
        return {"error": str(e), "ready_for_training": False}


# ============================================================================
# CLI √âTENDUE POUR L'ENTRA√éNEMENT GRANULAIRE
# ============================================================================

def cli_force_granular_retrain():
    """Commande CLI pour forcer le r√©-entra√Ænement granulaire"""
    
    print("üî• FOR√áAGE DU R√â-ENTRA√éNEMENT GRANULAIRE")
    print("="*50)
    
    # V√©rifier les prompts d'abord
    print("üîç V√©rification des prompts granulaires...")
    prompts_status = check_granular_prompts_integration()
    
    if prompts_status.get("error"):
        print(f"‚ùå Erreur prompts: {prompts_status['error']}")
        return False
    
    print(f"‚úÖ Prompts trouv√©s: {prompts_status['prompts_found']}")
    print(f"‚ö†Ô∏è Prompts manquants: {prompts_status['prompts_missing']}")
    print(f"üìä Compl√©tude: {prompts_status.get('completeness', 0):.1%}")
    
    for category, count in prompts_status.get('granular_coverage', {}).items():
        if count > 0:
            print(f"  üìã {category}: {count} prompts")
    
    if prompts_status["prompts_found"] == 0:
        print("‚ùå Aucun prompt trouv√© - V√©rifiez le dossier prompts/")
        return False
    
    # Confirmation
    response = input(f"\n‚ùì Forcer le r√©-entra√Ænement granulaire avec {prompts_status['prompts_found']} prompts? (y/N): ")
    if response.lower() != 'y':
        print("‚ö†Ô∏è R√©-entra√Ænement granulaire annul√©")
        return False
    
    # Lancer le r√©-entra√Ænement granulaire
    success = force_granular_retrain()
    
    if success:
        print("\nüéâ R√©-entra√Ænement granulaire termin√© avec succ√®s!")
        print("üí° Syst√®me pr√™t pour g√©n√©ration granulaire IA")
        return True
    else:
        print("\n‚ùå R√©-entra√Ænement granulaire √©chou√©")
        return False


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "force-granular":
            success = cli_force_granular_retrain()
            sys.exit(0 if success else 1)
            
        elif command == "check-granular":
            status = check_granular_prompts_integration()
            print(f"üéØ Statut granulaire: {status}")
            sys.exit(0)
            
        elif command == "quick-granular":
            print("üöÄ R√©-entra√Ænement granulaire rapide...")
            success = force_granular_retrain()
            print("‚úÖ Termin√©!" if success else "‚ùå √âchec")
            sys.exit(0 if success else 1)
    
    # Usage par d√©faut
    print("üåä Enhanced Training Manager")
    print("="*60)
    print("Commandes disponibles:")
    print("  python enhanced_training_v2.py force-granular")
    print("  python enhanced_training_v2.py check-granular") 
    print("  python enhanced_training_v2.py quick-granular")
    print()
    print("Int√©gration dans votre code:")
    print("  from enhanced_training_v2 import force_granular_retrain")
    print("  success = force_granular_retrain()")
